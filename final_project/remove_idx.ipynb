{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b020c2",
   "metadata": {},
   "source": [
    "\n",
    "# Word2Vec and Seq2Seq Project\n",
    "\n",
    "This notebook is designed to use a Word2Vec model to process textual data, followed by a sequence-to-sequence (Seq2Seq) neural network architecture.\n",
    "The Seq2Seq model will be trained to predict article titles from abstracts, transforming each input sequence of word embeddings into a target sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9032e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Sample abstract: pakistan ambassador u said government would endorse separate inquiry modeled one carried u n assassination rafik hariri lebanon 2005\n",
      "Sample title: reversal pakistan welcome outside help inquiry bhutto\n",
      "Sample keywords: ['assassination attempted assassination', 'pakistan', 'bhutto benazir', 'federal bureau investigation', 'united nation']\n"
     ]
    }
   ],
   "source": [
    "# Load the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load data (assuming a similar CSV file with 'abstract' and 'title' columns)\n",
    "articles = pd.read_csv('../final_project/NYT/NYT_Dataset_Preprocessed.csv')\n",
    "\n",
    "articles = articles[articles['preprocessed_abstract'] != '']\n",
    "articles = articles[articles['preprocessed_title'] != '']\n",
    "articles = articles[articles['preprocessed_keywords'] != '']\n",
    "\n",
    "articles = articles[(articles['preprocessed_abstract'].apply(type) == str) &\n",
    "                    (articles['preprocessed_title'].apply(type) == str)]\n",
    "\n",
    "abstracts = articles['preprocessed_abstract']\n",
    "titles = articles['preprocessed_title']\n",
    "keywords = articles['preprocessed_keywords']\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(f\"Sample abstract: {abstracts.iloc[0]}\")\n",
    "print(f\"Sample title: {titles.iloc[0]}\")\n",
    "print(f\"Sample keywords: {keywords.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8463afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize abstracts for Word2Vec training\n",
    "abstracts_tokenized = [word_tokenize(abstract.lower()) for abstract in abstracts]\n",
    "\n",
    "# \"Train\" Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=abstracts_tokenized, vector_size=100, window=5, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96d4ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Removed embedding layer as we are already working with precomputed embeddings\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # No need for embedding, just pass the input (already embeddings) to LSTM\n",
    "        outputs, (hidden, cell) = self.lstm(src)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, trg, hidden, cell):\n",
    "        # trg is expected to already be embeddings, so skip nn.Embedding\n",
    "        print(f\"Input to LSTM (trg): {trg.shape}\")  # Debug shape\n",
    "        outputs, (hidden, cell) = self.lstm(trg, (hidden, cell))\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        print(f\"in foward: src shape = {src.shape}, trg shape = {trg.shape}, trg.dim = {trg.dim()}, src.dim = {src.dim()}\")\n",
    "        hidden, cell = self.encoder(src)\n",
    "        outputs, _, _ = self.decoder(trg, hidden, cell)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "max_len = 50\n",
    "word2vec_model = Word2Vec(vector_size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text, word2vec, max_len):\n",
    "    pad_vector = np.zeros(word2vec.vector_size)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    embeddings = [word2vec.wv[token] if token in word2vec.wv else pad_vector for token in tokens]\n",
    "    if len(embeddings) < max_len:\n",
    "        embeddings += [pad_vector] * (max_len - len(embeddings))  # Pad to max_len\n",
    "    return np.array(embeddings[:max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_abstracts = [embed_text(abstract, word2vec_model, max_len) for abstract in abstracts]\n",
    "embedded_keywords = [embed_text(keyword, word2vec_model, max_len) for keyword in keywords]\n",
    "embedded_titles = [embed_text(title, word2vec_model, max_len) for title in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded titles shape: torch.Size([105883, 50, 100])\n",
      "Embedded abstracts shape: torch.Size([105883, 50, 100])\n",
      "Embedded keywords shape: torch.Size([105883, 50, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jackh\\AppData\\Local\\Temp\\ipykernel_35564\\2802251749.py:3: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  embedded_titles = torch.tensor(embedded_titles, dtype=torch.long)\n",
      "C:\\Users\\jackh\\AppData\\Local\\Temp\\ipykernel_35564\\2802251749.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  embedded_titles = torch.tensor(embedded_titles, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "embedded_titles = torch.tensor(embedded_titles, dtype=torch.long)\n",
    "embedded_abstracts = torch.tensor(embedded_abstracts, dtype=torch.float32)\n",
    "embedded_keywords = torch.tensor(embedded_keywords, dtype=torch.float32)\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"Embedded titles shape:\", embedded_titles.shape)  # (num_samples, max_len)\n",
    "print(\"Embedded abstracts shape:\", embedded_abstracts.shape)  # (num_samples, max_len, embedding_dim)\n",
    "print(\"Embedded keywords shape:\", embedded_keywords.shape)  # (num_samples, max_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at this\n",
    "def build_vocab(texts, dictionary):\n",
    "    for text in texts:\n",
    "        for word in word_tokenize(text.lower()):\n",
    "            dictionary[word] += 1\n",
    "    return dictionary\n",
    "    # return {word: idx + 1 for idx, (word, _) in enumerate(dictionary.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = defaultdict(int)\n",
    "\n",
    "word_freq = build_vocab(abstracts, word_freq)\n",
    "word_freq = build_vocab(titles, word_freq)\n",
    "word_freq = build_vocab(keywords, word_freq)\n",
    "word2idx = {word: idx + 1 for idx, (word, _) in enumerate(word_freq.items())}\n",
    "\n",
    "# Define the sizes (replace these with your specific values)\n",
    "input_size = 200  # Size of the input vocabulary\n",
    "output_size = len(word2idx) + 1  # Size of the output vocabulary\n",
    "hidden_size = 512  # Size of the hidden layer (common choice)\n",
    "\n",
    "# Initialize the Encoder and Decoder\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "\n",
    "# Initialize the Seq2Seq model\n",
    "seq2seq_model = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Concatenate embedded abstracts and keywords to form input data\n",
    "input_data = torch.cat((embedded_abstracts, embedded_keywords), dim=-1)\n",
    "indices = torch.arange(input_data.size(0))\n",
    "\n",
    "# Split the data into train and test sets (80% train, 20% test)\n",
    "train_inputs, test_inputs, train_titles, test_titles, train_indices, test_indices = train_test_split(input_data, embedded_titles, indices, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe597325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(seq2seq_model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "\n",
    "# Calculate the number of batches based on batch size\n",
    "num_batches = len(train_inputs) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0  # Track epoch loss\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        # Get batch of data\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(train_inputs))\n",
    "        \n",
    "        # Select batch from tensors\n",
    "        src = train_inputs[start_idx:end_idx, :, :]  # Abstract and Keyword embeddings\n",
    "        print(train_titles.shape)\n",
    "        trg = train_titles[start_idx:end_idx, :, :]  # Title embedding\n",
    "\n",
    "        # Debugging: Print shapes before reshaping\n",
    "        print(f\"Before permute: src shape = {src.shape}, trg shape = {trg.shape}, trg.dim = {trg.dim()}, src.dim = {src.dim()}\")\n",
    "\n",
    "        # Reshape target for decoder\n",
    "        trg = trg.permute(1, 0, 2)  # Now (seq_len, batch_size, embedding_dim)\n",
    "\n",
    "        # Debugging: Print shapes after reshaping\n",
    "        print(f\"After permute: src shape = {src.shape}, trg shape = {trg.shape}, trg.dim = {trg.dim()}, src.dim = {src.dim()}\")\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = seq2seq_model(src, trg)\n",
    "        \n",
    "        # Compute loss and backpropagation\n",
    "        loss = criterion(output.view(-1, output_size), trg.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print average loss for each epoch\n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {avg_epoch_loss:.8f}\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(predicted_vectors, word2vec_model):\n",
    "    words = []\n",
    "    for vector in predicted_vectors:\n",
    "        # Find the closest word in the embedding space\n",
    "        closest_word = word2vec_model.wv.similar_by_vector(vector, topn=1)[0][0]\n",
    "        words.append(closest_word)\n",
    "    return words\n",
    "\n",
    "def decode_ground_truth(trg_embeddings, word2vec_model):\n",
    "    words = []\n",
    "    for vector in trg_embeddings:\n",
    "        closest_word = word2vec_model.wv.similar_by_vector(vector, topn=1)[0][0]\n",
    "        words.append(closest_word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ffb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "seq2seq_model.eval()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Calculate the number of batches based on batch size\n",
    "num_batches = len(test_inputs) // batch_size\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(seq2seq_model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize variables for tracking test loss and predictions\n",
    "test_loss = 0\n",
    "predicted_titles = []\n",
    "ground_truth_titles = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_test_batches = len(test_inputs) // batch_size\n",
    "    \n",
    "    for i in range(num_test_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(test_inputs))\n",
    "\n",
    "        src = test_inputs[start_idx:end_idx]\n",
    "        trg = test_titles[start_idx:end_idx]\n",
    "\n",
    "        # Forward pass\n",
    "        output = seq2seq_model(src, trg)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output.view(-1, output_size), trg.view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Decode predicted title vectors to words\n",
    "        for j in range(batch_size):\n",
    "            predicted_vectors = output[j].cpu().detach().numpy()\n",
    "            predicted_title = decode_predictions(predicted_vectors, word2vec_model)\n",
    "            predicted_titles.append(predicted_title)\n",
    "\n",
    "            # Decode ground truth titles to words\n",
    "            ground_truth_vectors = trg[j].cpu().detach().numpy()\n",
    "            ground_truth_title = decode_ground_truth(ground_truth_vectors, word2vec_model)\n",
    "            ground_truth_titles.append(ground_truth_title)\n",
    "\n",
    "# Calculate average test loss\n",
    "avg_test_loss = test_loss / num_test_batches\n",
    "print(f\"Average Test Loss: {avg_test_loss:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    index = test_indices[i].item()\n",
    "\n",
    "    print(f\"Sample {i+1}\")\n",
    "    print(\"Ground Truth Title:\", \" \".join(ground_truth_titles[i]))\n",
    "    print(\"Predicted Title:  \", \" \".join(predicted_titles[i]))\n",
    "    print(\"Abstract:  \", \"\".join(abstracts.iloc[index]))\n",
    "    print(\"Keywords:  \", \"\".join(keywords.iloc[index]))\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Specify the path where you want to save the model\n",
    "model_path = 'seq2seq_model.pth'\n",
    "\n",
    "# Save the model's state_dict\n",
    "torch.save({\n",
    "    'model_state_dict': seq2seq_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),  # If you want to save the optimizer as well\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path where the model was saved\n",
    "model_path = 'seq2seq_model.pth'\n",
    "\n",
    "# Initialize the model architecture\n",
    "seq2seq_model = Seq2Seq(encoder, decoder)\n",
    "optimizer = torch.optim.Adam(seq2seq_model.parameters())  # Initialize optimizer if saved\n",
    "\n",
    "# Load the saved state_dict into the model and optimizer\n",
    "checkpoint = torch.load(model_path)\n",
    "seq2seq_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Set the model to evaluation mode if you plan on evaluating\n",
    "seq2seq_model.eval()\n",
    "print(\"Model loaded and ready for evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
